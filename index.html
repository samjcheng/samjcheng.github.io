<!doctype html>
 
<html class="no-js" lang="en" >  
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="chrome=1"> 
	<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
	<link rel="icon" type="image/x-icon" href="favicon.ico">
	<link rel="Bookmark" type="image/x-icon" href="favicon.ico">
	<link rel="apple-touch-icon" type="image/x-icon" href="favicon.png">
	<link rel="stylesheet" href="style.css">
	
	<title>Homepage of Jun Cheng</title>
<!-- <meta name="viewport" content="width=device-width"> -->
</head>

<body>
	<div class="wrapper"> 
		<section>
			<table border="0" id="table1" width="100%">
				<tbody>
					<tr>
						

						<td style="width:60%">
							<h1> <a id="top-page" class="anchor" href="#top-page" aria-hidden="true"><span
								class="octicon octicon-link"></span></a>
								Jun Cheng</h1>
							<p>
								<b>Principal Scientist</b><br>
								<em> Institute for Infocomm Research<br>
								Agency for Science, Technology and Research (A*STAR), Singapore. </em>
							</p>
							<p>
								<highcolor><b>Email:</b></highcolor>  
								    <a href='mailto:cheng_jun@i2r.a-star.edu.sg'><em>cheng_jun(AT)i2r.a-star.edu.sg</em></a> 
								    <a href='mailto:juncheng@ieee.org'><em>juncheng(AT)ieee.org</em></a> <br>

							</p>  

						</td>

						<td>
  							<p align="left">
								<img border="0" src="jun.jpg" width="120">
							</p> 
						</td>

					</tr>
				</tbody>
			</table>

			<b>
				<a href="#research-interest">[<ud>Research Interest</ud>]</a> 
				- <a href="#positions">[<ud>Positions</ud>]</a>  
				- <a href="#Projects">[<ud>Projects</ud>]</a>  
				- <a href="#Recent-News">[<ud>News</ud>]</a>  
				- <a href="#Academic-Services">[<ud>Services</ud>]</a> 
				- <a href="#Awards-pages">[<ud>Recognitions</ud>]</a>  
				- <a href="#Selected-Publications">[<ud>Selected Publications</ud>]</a> 
				- <a href="https://scholar.google.com.sg/citations?user=42Oy5CYAAAAJ&hl=en" target="_blank">[<b><font color="#4285F4">G</font><font color="#DB4437">o</font><font color="#F4B400">o</font><font color="#4285F4">g</font><font color="#0F9D58">l</font><font color="#DB4437">e</font> Scholar</b>]</a> 
				
			</b>
			 

			<hr />
			Jun Cheng received the B. E. degree in electronic engineering and information science from the University of Science and Technology of China,
			and the Ph. D. degree from Nanyang Technological University, Singapore. He is now a principal scientist in the Institute for Infocomm
			Research, A*STAR, working on AI for medical imaging, robust vision & perception, and machine learning. He has authored/co-authored over 200 publications at prestigious 
			journals/conferences, such as TMI, TIP, TBME, IOVS, JAMIA, MICCAI, CVPR and invented more than 20 patents. He has received the IES Prestigious Engineering Achievement Award 2013.
			He serves as reviewers for many journal/conferences and area chairs for MICCAI, AAAI, ICLR, NeurIPS. He is currently associate editor for IEEE TIP and IEEE TMI. 
			<br>
			 
<!-- 			 <h1> <hl_red>We are Hiring: Multiple Positions for  <a href="https://www.linkedin.com/jobs/view/3600833176/"
								target="_blank">Research Scientist (3D Vision)</a> 
				 </hl_red> </li>
			</h1>
			
                        <h3> <hl_red>Call for paper: </hl_red> <a href="https://www.nowpublishers.com/Public-Content/CFP_ATSIP_AI_Healthcare_2023.pdf"
								target="_blank">Themed Series of APSIPA Trans. on Signal and Information Processing on
                          “AI for Healthcare”</a></li>
			</h3> -->
			<h3>
				<a id="research-interest" class="anchor" href="#research-interest" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>
				<highcolor>Research Interest:</highcolor>
			</h3>
				<li>
					<strong>Computer Vision:</strong> 3D Vision, Robust Vision and Perception

				</li>
				<li>
					<strong>Medical Image Computing:</strong> Medical Image Registration, Medical Imaging, Medical Image Segmentation
				</li>
				<li>    
					<strong>Machine Learning:</strong> Unserpervised Learning, Multimodal Data Learning, Learning With Less Data, Physical-driven Deep Learning
				</li>
                          <div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>

			
                        <hr /> 	

			<h3>
				<a id="positions" class="anchor" href="#positions" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>
				<highcolor>Open Positions:</highcolor>
			</h3>
		
				<li>
					 <hl_red>Chinese Government Scholarship: </hl_red>
					<a href="https://www.csc.edu.cn/chuguo" target="_blank"><b>Chinese CSC Scholarships</b> </a>  
					support exchange PhD students from China.

				</li>  
			         <li>
					<hl_red>A*STAR Graduate Scholarship: </hl_red>
					<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/a-star-graduate-scholarship-singapore" 
					   target="_blank"><b>A*STAR Graduate Scholarship</b></a> supports Singapore students who wish to pursue their PhDs from Singapore Universities (NUS, NTU, and SUTD). 
				</li>

				<li>
					<hl_red>A*STAR SINGA: </hl_red>
					<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/singapore-international-graduate-award-singa" 
					   target="_blank"><b>A*STAR SINGA Scholarship</b></a> supports international students who wish to pursue their PhDs from Singapore Universities (NUS, NTU, and SUTD). 
				</li>

				<li>   <hl_red>A*STAR ARAP: </hl_red>
					<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/singapore-international-graduate-award-singa" 
					   target="_blank"><b>A*STAR Research Attachment Programme</b></a> support visiting international PhD students.

				</li>  
				<li>   
					<hl_red>A*STAR SIPGA: </hl_red>
					<a href="https://www.a-star.edu.sg/Scholarships/for-undergraduate-studies/singapore-international-pre-graduate-award-sipga" 
					   target="_blank"><b>A*STAR SIPGA</b></a> The Singapore International Pre-Graduate Award (SIPGA) supports short-term 
					research attachments for top international Undergraduate & Master students at A*STAR. 
				</li>
			        <li>   <hl_red>Research Interns: </hl_red>
					Looking for research intern students for projects.

				</li>  
			  <div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>

			<hr />
 
			<h3>
				<a id="Projects" class="anchor" href="#Projects" aria-hidden="true"><span
						class="octicon octicon-link"></span></a> Projects:
			</h3>

			<ul> 
				<li><b>Co-Principal Investigator</b>, "Towards Realisitic Deep Learning for 3D Vision", <em>MTC Programmatic Fund</em>, 2023-2026.
				</li>

				<li><b>Principal Investigator</b>, "AI-based OCT Angiography for Non-Invasive Capillary Blood Flow Imaging", <em>A*STAR AI3 HTPO Seed Fund</em>, 2023-2024 </li>
				<li><b>Co-Principal Investigator</b>, "Weakly Supervised Learning by Joint Convolutional and Graph Networks for Non-linear Medical Image Registration", <em>A*STAR AI3 HTPO Seed Fund</em>, 2022-2023. 
				</li> 

				<li><b>Co-Principal Investigator</b>, "Spatial Transcriptomics in Conjuction with Graph Neural Networks for Cell-Cell Interaction", <em>A*STAR AI3 HTPO Seed Fund</em>, 2022-2024.
				</li>
				<li><b>Principal Investigator</b>, "Object Detection in Robotics", <em>UBTech Research</em>, 2019-2020.
				</li>
				<li><b>Principal Investigator</b>, "ACTIA: Automatic Cardiac Optical Coherence Tomography Image Analysis for Coronary Artery Disease Risk Assessment", <em>A*STAR BEP</em>, 2015-2017.
				</li>
			</ul>

			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>

				
			<hr /> 	

			
			<h3>
				<a id="Recent-News" class="anchor" href="#Recent-News" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>
				<highcolor>Recent News:</highcolor>
			</h3>

			<ul>    
			        <li>2024-03: Be invited to serve as Area Chair for NeurIPS 2024. 
				<li>2024-02: One paper accepted by CVPR 2024. 
				<li>2023-12: One paper accepted by AAAI 2024.
				<li>2023-10: Happy to be <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6" target="_blank"><hl_red>Top 2% Scientists Worldwide</hl_red></a> in the area of artificial intelligence & image processing, identified by Stanford University, 
					2023. 
				<li>2023-10: One paper accepted by IJCV.
				<li>2023-09: Be invited to serve as Area Chair for ICPR 2024. 
				<li>2023-08: Be invited to serve as Area Chair for ICLR 2024.
				<li>2023-07: Happy to be co-PI of MTC Programmatic Funds：Towards Realisitic Deep Learning for 3D Vision.
				<li>2023-07: One paper accepted by ICCV 2023.
				<li>2023-06: Happy to be PI of A*STAR AI3 HTPO Seed Fund.
				<li>2023-04: Call for paper: <a href="https://www.nowpublishers.com/Public-Content/CFP_ATSIP_AI_Healthcare_2023.pdf"
								target="_blank">Themed Series of APSIPA Trans. on Signal and Information Processing on
“AI for Healthcare”</a></li>
				
				<li>2023-03: One student has been awarded by <a href="https://www.a-star.edu.sg/Scholarships/for-undergraduate-studies/singapore-international-pre-graduate-award-sipga" target="_blank"><hl_red>Singapore International Pre-Graduate Award.</a> 
				<li>2023-03: One paper "Perceptual Quality Assessment of Enhanced Colonoscopy Images: A Benchmark Dataset and An Objective Method" has been accepted by TCSVT.
				<li>2023-03: Be invited to serve as Area Chair for NeurIPS 2023.
				<li>2023-02: One student has been awarded by <a href="https://www.a-star.edu.sg/Scholarships/for-undergraduate-studies/singapore-international-pre-graduate-award-sipga" target="_blank"><hl_red>Singapore International Pre-Graduate Award.</a>
				<li>2023-01: Be invited to serve as Area Chair for MICCAI 2023.
				<li>2023-01: One paper "CLC-Net: Contextual and Local Collaborative Network for Lesion Segmentation in Diabetic Retinopathy Images" has been accepted by Neurocomputing.
				<li>2022-12: One paper "CPP-Net: Context-aware Polygon Proposal Network for Nucleus Segmentation" has been accepted by TIP.
				<li>2022-10: Happy to be <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw" target="_blank"><hl_red>Top 2% Scientists Worldwide</hl_red></a> in the area of artificial intelligence & image processing, identified by Stanford University, 
					2022. 
				<li>2022-09: One paper "RA Loss: Relation-Aware Loss for Robust Person Re-identification" has been accepted by ACCV.
				<li>2022-08: Be invited to serve as Area Chair for ICLR 2023
				<li>2022-07: Be invited to serve as Associate Editor for IEEE TIP.
				<li>2022-01: Be invited to serve as Area Chair for MICCAI 2022. 
			
			</ul>
		        </div>
			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />


			<h3>
				<a id="Academic-Services" class="anchor" href="#Academic-Services" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>Academic Services:
			</h3>
			<ul>
				<li><strong>Memberships:</strong>
					<ul>
						<li> IEEE Senior Member. </li>
				</ul> 
				</li>

				<li><strong>Associate Editor:</strong>
					<ul>
						<li> IEEE Transactions on Medical Imaging (<strong>IEEE TMI</strong>), 2015 - present. </li>
						<li> IEEE Transactions on Image Processing(<strong>IEEE TIP</strong>), 2022 - present. </li>
						
					</ul>
				</li>

				<li><strong>Guest Editor:</strong>
					<ul>
						
						<li> IEEE JBHI Special issue on “<i>Ophthalmic Image Analysis and Informatics</i>”, 2020.
							[<a href="https://www.embs.org/jbhi/articles/special-issue-on-ophthalmic-image-analysis-and-informatics/"
								target="_blank">Link</a>]</li>
					</ul>
				</li>

				<li><strong>Area Chair/Senior-PC:</strong>
					<ul>
						<li>
							MICCAI (2019, 2021, 2022,2023),  AAAI (2022), ICLR (2023, 2024), NeurIPS (2023), ICPR （2024).
						</li>
					</ul>
				</li>


				<li><strong>Chair: </strong>
					<ul>
						<li>MICCAI Workshop, 2022 
							[<a href="https://conferences.miccai.org/2022/en/" target="_blank">Link</a>] </li>
					</ul>
				</li>
			</ul>
			</p>

			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />

			<h3>
				<a id="Awards-pages" class="anchor" href="#Awards-pages" aria-hidden="true" ><span class="octicon octicon-link"></span></a> Recognitions: 
			</h3>
			<ul> 				
				<li>   <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw" target="_blank">'Top 2% Scientists Worldwide'</a>, identified by Stanford University, 2022-2023.
				       
				</li>
				<li>
					IEEE TMI Distinguished Reviewer, 2021, 2022.
				</li>
				<li>
					IES Prestigious Engineering Achievement Award, 2013
				</li>
				
			</ul>
			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />

			<h3>
				<a id="Selected-Publications" class="anchor" href="#Selected-Publications" aria-hidden="true" ><span class="octicon octicon-link"></span></a> Selected Publications: 
			</h3>

				<em>See more publications in:</em> 
				<a href="https://scholar.google.com.sg/citations?user=42Oy5CYAAAAJ&hl=en" target="_blank">[<b><font color="#4285F4">G</font><font color="#DB4437">o</font><font color="#F4B400">o</font><font color="#4285F4">g</font><font color="#0F9D58">l</font><font color="#DB4437">e</font> Scholar</b>]</a>  

			<ul>
				<li><em>Rui Gong, Weide Liu, Zaiwang Gu, Xulei Yang,  Jun Cheng*,  </em><br>
				<strong>Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching,
				</strong></a> <br>
					CVPR, 2024.
				 </li>	
 				<li><em>Yu Wang, Xiaoye Wang, Zaiwang Gu, Weide Liu, Wee Siong Ng, Weimin Huang, Jun Cheng*,  </em><br>
				<strong>SuperJunction: Learning-based Junction Detection for Retinal Image Registration,
				</strong></a> <br>
					AAAI, 2024.
				 </li>	
	                        <li><em>Weide Liu, Zhonghua Wu, Yang Zhao, Yuming Fang, Chuan Sheng Foo, Jun Cheng, Guosheng Lin, </em><br>
			                <a href="https://arxiv.org/pdf/2303.13724.pdf target="_blank">
					<strong>Harmonizing Base and Novel Classes: A Class Contrastive Approach for Generalized Few-Shot Segmentation,
				</strong></a> <br>
					International Journal on Computer Vision, 2023.
				 
				 </li>				
				<li><em>Jieming Lou, Weide Liu, Zhuo Chen, Fayao Liu,  Jun Cheng*,  </em><br>
			                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lou_ELFNet_Evidential_Local-global_Fusion_for_Stereo_Matching_ICCV_2023_paper.html" target="_blank">
					<strong>ELFNet: Evidential Local-global Fusion for Stereo Matching,
				</strong></a> <br>
					International Confernece on Computer Vision, 2023.
				<a href="https://github.com/jimmy19991222/ELFNet" target="_blank">[Code]</a>
				 </li>
				
				<li><em>Guanghui Yue, Di Cheng, Tianwei Zhou, Jingwen Hou, Weide Liu, Long Xu, Tianfu Wang,  Jun Cheng,  </em><br>
					<a href="https://ieeexplore.ieee.org/document/10078370" target="_blank">
					<strong>Perceptual Quality Assessment of Enhanced Colonoscopy Images: A Benchmark Dataset and An Objective Method,
				</strong></a> <br>
					IEEE Transactions on Circuits and Systems for Video Technology, 2023.
				 </li>
					
				<li><em>Kang Zhou, Jing Li, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen Liu, Weixin Luo, Jiang Liu, Shenghua Gao,</em><br>
					<a href="https://ieeexplore.ieee.org/document/9513473" target="_blank">
					<strong>Memorizing Structure-Texture Correspondence for Image Anomaly Detection,
				</strong></a> <br>
				 IEEE Transactions on Neural Networks and Learning Systems (TNNLS), vol. 33, no. 6, pp. 2335-2349, 2022. <br>	
				 
			        </li>

				<li><em>Yepeng Liu, Zaiwang Gu, Shenghua Gao, Dong Wang, Yusheng Zeng, Jun Cheng*,</em><br>
					<a href="https://www.bmvc2021-virtualconference.com/assets/papers/0580.pdf" target="_blank">
					<strong>MOS：A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation,
				</strong></a> <br>
					BMVC, 2021. <br>			
				<a href="https://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect" target="_blank">[Code]</a>
			        </li>
				
				
				<li><em>Tianyang Zhang, Jun Cheng*, Huazhu Fu, Zaiwang Gu, Yuting Xiao, Kang Zhou, Shenghua Gao, Rui Zheng, Jiang Liu,</em><br>
					<a href="https://ieeexplore.ieee.org/document/8852672" target="_blank">
					<strong>Noise Adaptation Generative Adversarial Network for Medical Image Analysis</strong></a> <br>
					 IEEE Trans. On Medical Imaging (TMI), vol. 39, no. 4, pp. 1149-1159, 2020.
				<br>
				<a href="https://github.com/NeilZhang-IMED/NAGAN" target="_blank">[Code]</a>
			        </li>
				
				<li><em>Kang Zhou, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen Liu, Weixin Luo, Zaiwang Gu, Jiang Liu, Shenghua Gao,</em><br>
					<a href="https://arxiv.org/pdf/2008.03632.pdf" target="_blank">
					<strong>Encoding Structure-Texture Relation with P-Net for Anomaly Detection in Retinal Images</strong></a> <br>
				European Conference on Computer Vision (ECCV), 2020. <br>
				<a href="https://github.com/Kevin-KangZhou/P_Net_Anomaly_Detection" target="_blank">[Code]</a>
			        </li>
				
				<li><em>Kang Zhou, Jing Li, Weixin Luo, Zhengxin Li, Jianlong Yang, Huazhu Fu, Jun Cheng, Jiang Liu, Shenghua Gao,</em><br>
					<a href="https://arxiv.org/pdf/2110.01761.pdf" target="_blank">
					<strong>Proxy-bridged Image Reconstruction Network for Anomaly Detection in Medical Images</strong></a> <br>
					 IEEE Trans. On Medical Imaging (TMI), vol. 38, no. 10, pp. 2281-2292, 2019.<br>	
					<a href="https://github.com/samjcheng/CE-Net" target="_blank">[Code]</a>
			        </li>
				
				<li><em>Jimmy Addison Lee, Peng Liu, Jun Cheng* and Huazhu Fu,</em><br>
					<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_A_Deep_Step_Pattern_Representation_for_Multimodal_Retinal_Image_Registration_ICCV_2019_paper.pdf" target="_blank">
					<strong>A Deep Step Pattern Representation for Multimodal Retinal Image Registration</strong></a> <br>
					  in IEEE International Conference in Computer Vision (ICCV), 2019.
				<br>
				 
			        </li>
				
					<li><em>Zaiwang Gu, Jun Cheng*, Kang Zhou, Huaying Hao, Yitian Zhao, Tianyang Zhang, Shenghua Gao, Jiang Liu,</em><br>
					<a href="https://arxiv.org/abs/1903.02740.pdf" target="_blank">
					<strong>CE-Net: Context Encoder Network for 2D Medical Image Segmentation</strong></a> <br>
					 IEEE Trans. On Medical Imaging (TMI), vol. 38, no. 10, pp. 2281-2292, 2019.
				<br>
				<a href="https://github.com/samjcheng/CE-Net" target="_blank">[Code]</a>
						(<hl_red><em>ESI Highly Cited Paper</em></hl_red>, 
					<a href="https://scholar.google.com/citations?hl=en&view_op=list_hcore&venue=wqLkMlos2DIJ.2022" target="_blank"><hl_red><em>Top-20 Most Cited Paper within 5 Years in IEEE TMI 2022</em></hl_red></a>,
					<a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=42" target="_blank"><hl_red><em>Top-50 Most Frequently Accessed Documents in IEEE TMI"</em></hl_red></a>)
			       
				</li>
				
				<li><em>Jun Cheng*, Zhengguo Li, Zaiwang Gu, Huazhu Fu, Damon Wing Kee Wong, Jiang Liu,</em><br>
					<a href="https://arxiv.org/abs/1805.06625.pdf" target="_blank">
					<strong>Structure-preserving Guided Retinal Image Filtering and Its Application for Optic Disc Analysis</strong></a> <br>
					IEEE Transactions on Medical Imaging (TMI), vol. 37, no. 11, pp. 2536-2546, 2018.
				<br>
				<a href="https://github.com/samjcheng/structure-preserving-guided-retinal-image-filtering" target="_blank">[Code]</a>
			        </li>
								
				
				<li><em>Huazhu Fu, Jun Cheng, Yanwu Xu, Damon Wing Kee Wong, Jiang Liu, Xiaochun Cao,</em><br>
					<a href="https://arxiv.org/abs/1801.00926.pdf" target="_blank">
					<strong>Joint Optic Disc and Cup Segmentation Based on Multi-label Deep Network and Polar Transformation</strong></a> <br>
					IEEE Transactions on Medical Imaging (TMI), vol. 37, no. 7, pp. 1597–1605, 2018.
				<br>
				<a href="https://github.com/HzFu/MNet_DeepCDR" target="_blank">[Code]</a>
					(<hl_red><em>ESI Highly Cited Paper</em></hl_red>, 
					<a href="https://scholar.google.com/citations?hl=en&view_op=list_hcore&venue=wqLkMlos2DIJ.2022" target="_blank"><hl_red><em>Top-20 Most Cited Paper within 5 Years in IEEE TMI 2022</em></hl_red></a>)
			        </li>
				
				
				<li><em>Huazhu Fu, Jun Cheng*, Yanwu Xu, Changqing Zhang, Damon Wing Kee Wong, Jiang Liu, Xiaochun Cao,</em><br>
					<a href="https://arxiv.org/abs/1805.07549.pdf" target="_blank">
					<strong>Disc-aware Ensemble Network for Glaucoma Screening from Fundus Image,</strong></a> <br>
					EEE Transactions on Medical Imaging (TMI), vol. 37, no. 11, pp. 2493–2501, 2018.
				<br>
				<a href="https://github.com/HzFu/DENet_GlaucomaScreen" target="_blank">[Code]</a>
			        </li>
				
				
		
			</ul> 

			 

			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />
  
	 
		 
<a href="https://www.revolvermaps.com/livestats/5n6xr5v55ws/"><img src="//rf.revolvermaps.com/h/m/a/0/ff0000/128/0/5n6xr5v55ws.png" width="256" height="128" alt="Map" style="border:0;"></a>

	
		</section>

	</div>
</body>

</html>
